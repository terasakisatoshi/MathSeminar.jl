<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
   <link rel="stylesheet" href="/MathSeminar.jl/libs/highlight/github.min.css">
   
  

  <link href="/MathSeminar.jl/css/franklin.css" rel="stylesheet">
  <link href="/MathSeminar.jl/css/vela.css" rel="stylesheet">
  <script src="/MathSeminar.jl/libs/vela/jquery.min.js"></script>
  <link rel="stylesheet" href="/MathSeminar.jl/css/theorem.css">
   <title>PyCallChainRules.jl 入門</title>  
  <!-- PLOTLY -->
  

  <title>Franklin Example | Vela</title>
</head>
<body>
  <div class="main-nav" id="menu">
    <div class="flex-container">
      <span class="sidebar-brand">
        <h3 style='font-size: 25px'>MathSeminar.jl</h3>
      </span>
    </div> <!-- class="flex-container" -->

    <nav class="sidebar-nav">
      <ul class="metismenu"  id="metismenu" >
        <li><a href="/MathSeminar.jl/index.html">Home</a></li>
        <li><a href="" class="has-arrow">Slide Show</a>
          <ul>
            <li><a href="/MathSeminar.jl/slideshow/remark/build/">Introduction to Remark.jl</a>
            <li><a href="/MathSeminar.jl/slideshow/franklin/build">Introduction to Franklin.jl</a>
            <li><a href="/MathSeminar.jl/slideshow/dash/build">Introduction to Dash.jl</a>
            <li><a href="/MathSeminar.jl/slideshow/binarybuilder/build">Introduction to BinaryBuilder.jl</a>
            <li><a href="/MathSeminar.jl/slideshow/myworkflow/build">Introduction to MyWorkflow.jl</a>
            <li><a href="/MathSeminar.jl/slideshow/repl/build">Do something great in your REPL</a>
            <li><a href="/MathSeminar.jl/slideshow/intro2julia/build">Into2Julia</a>
            <li><a href="/MathSeminar.jl/slideshow/pycallchainrules/build">PyCallChainRules</a>
          </ul>
        </li>
        <li><a href="" class="has-arrow">Programming</a>
          <ul>
            <li><a href="/MathSeminar.jl/programming/clang">C</a></li>
            <li><a href="/MathSeminar.jl/programming/rust">Rust</a></li>
            <li><a href="/MathSeminar.jl/programming/benchmark_part1"> Julia benchmark Part1 </a></li>
            <li><a href="/MathSeminar.jl/programming/benchmark_part2"> Julia benchmark Part2 </a></li>
            <li><a href="/MathSeminar.jl/programming/literatejl">Literate.jl</a></li>
            <li><a href="/MathSeminar.jl/programming/sympy">SymPy.jl</a></li>
            <li><a href="/MathSeminar.jl/programming/franklin">Franklin.jl</a></li>
            <li><a href="/MathSeminar.jl/programming/weave">Weave.jl</a></li>
            <li><a href="/MathSeminar.jl/programming/theoremcounter">Theorem Counter</a></li>
            <li><a href="/MathSeminar.jl/programming/images">Images.jl</a></li>
            <li><a href="/MathSeminar.jl/programming/rcall">RCall.jl</a></li>
            <li><a href="/MathSeminar.jl/programming/asciinema">asciinema</a></li>
          </ul>
        </li>
        <li><a href="" class="has-arrow">Gallery</a>
          <ul>
            <li><a href="/MathSeminar.jl/gallery/animation/">Plots.jl animation</a></li>
            <li><a href="/MathSeminar.jl/gallery/curve/">Plots.jl curve</a></li>
            <li><a href="/MathSeminar.jl/gallery/gr/">GR.jl</a></li>
            <li><a href="/MathSeminar.jl/gallery/plotlyjs1/">PlotlyJS.jl Part1</a></li>
            <li><a href="/MathSeminar.jl/gallery/plotlyjs2/">PlotlyJS.jl Part2</a></li>
            <li><a href="/MathSeminar.jl/gallery/plotlyjs3/">PlotlyJS.jl Part3</a></li>
            <li><a href="/MathSeminar.jl/gallery/jsxgraph_js/">JSXGraph (JS)</a></li>
            <li><a href="/MathSeminar.jl/gallery/jsxgraph_jl/">JSXGraph (Julia)</a></li>
            <li><a href="/MathSeminar.jl/gallery/wglmakie/">WGLMakie.jl</a></li>
          </ul>
        </li>
        <li><a href="" class="has-arrow">Topics in Mathematics</a>
          <ul>
            <li><a href="/MathSeminar.jl/mathematics/linearalgebra/chapter0">Linear Algebra</a></li>
            <li><a href="/MathSeminar.jl/mathematics/statistics/dice">Dice</a></li>
            <li><a href="/MathSeminar.jl/mathematics/statistics/Chisq">Chisq Distributions</a></li>
            <li><a href="/MathSeminar.jl/mathematics/statistics/Law_of_Large_Numbers">Law_of_Large_Numbers</a></li>
            <li><a href="/MathSeminar.jl/mathematics/visualizations/triangular_function_addition_theorem">Triangular function addition theorem</a></li>
          </ul>
        </li>
      </ul>
    </nav>
  </div> <!-- main nav menu -->

  <main id="panel">
    <div class="toggle-button hamburger hamburger--spin">
      <div class="hamburger-box">
        <div class="hamburger-inner"></div>
      </div>
    </div>
    <h1 class="page title">PyCallChainRules.jl 入門</h1>
    <hr>


<!-- Content appended here -->
<div class="franklin-content"><p>class: center, middle</p>
<h1 id="pycallchainrulesjl_入門"><a href="#pycallchainrulesjl_入門" class="header-anchor">PyCallChainRules.jl 入門</a></h1>
<p>Flux.jl meets PyTorch</p>
<hr />
<h1 id="前置き"><a href="#前置き" class="header-anchor">前置き</a></h1>
<ul>
<li><p>ご存知の通り&#40;?&#41; Julia には Flux.jl という機械学習ライブラリがある. DeepLearning を行うための基本的なレイヤーが揃っている. でもなぜかあまりユーザーがいない.</p>
<ul>
<li><p>A「だって，Python 便利だし」</p>
<ul>
<li><p>私「はい．．．」</p>
</li>
</ul>
</li>
<li><p>B「みんな使ってるし PyTorch が便利なんだもーん」</p>
<ul>
<li><p>私「はい．．．」</p>
</li>
</ul>
</li>
<li><p>C「機能いっぱい揃ってるしエコシステムが充実」</p>
<ul>
<li><p>私「はい．．．」</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr />
<h1 id="pycall_がある"><a href="#pycall_がある" class="header-anchor">PyCall がある（？！）</a></h1>
<ul>
<li><p>私「 PyCall.jl はそんな便利なライブラリを呼び出せる！ したがって Python は実質 Julia のライブラリです」</p>
<ul>
<li><p>A 「ほほー，まぁ Torch.jl はあるらしいけれど動くんか？」</p>
</li>
</ul>
</li>
<li><p>私 「Torch.jl の更新頻度は低く，GPU 環境でしか動作できないのでデバッグ大変で」</p>
<ul>
<li><p>B 「まぁええわ．<code>torch &#61; pyimport&#40;&quot;torch&quot;&#41;</code> でインポートして <code>torch.nn.Linear</code> を使えるんか」</p>
</li>
</ul>
</li>
<li><p>私 「さようでございます」</p>
<ul>
<li><p>C 「機械学習ちゅーんやから学習したいわな．んで微分計算どうするん？」</p>
</li>
</ul>
</li>
<li><p>私 「Flux.jl が使う自動微分ライブラリ Zygote.jl は ChainRules.jl/ChainRulesCore.jl にある微分ルールをベースに微分演算を可能にします．ニューラルネットワークの微分ルールはNNlib.jl などで管理されており ChainRulesCore の拡張機能によってそれを可能にしています.」</p>
<ul>
<li><p>D 「はーん？なら Python のオブジェクトに関しての微分ルールがないと無理ちゃうんか？」</p>
</li>
</ul>
</li>
<li><p>私 「ぐーのねもでない」</p>
</li>
</ul>
<hr />
<p>class: center, middle</p>
<h1 id="ところがどっこい"><a href="#ところがどっこい" class="header-anchor">ところがどっこい</a></h1>
<p><a href="https://github.com/rejuvyesh/PyCallChainRules.jl">PyCallChainRules.jl</a></p>
<hr />
<h1 id="pycallchainrulesjl"><a href="#pycallchainrulesjl" class="header-anchor">PyCallChainRules.jl </a></h1>
<blockquote>
<p>While Julia is great, there are still a lot of existing useful differentiable python code in PyTorch, Jax, etc. Given PyCall.jl is already so great and seamless, one might wonder what it takes to differentiate through those pycalls. This library aims for that ideal.</p>
</blockquote>
<h1 id="つまり"><a href="#つまり" class="header-anchor">つまり?</a></h1>
<p>PyTorch のレイヤーを Julia から使える．それだけではなく Zygote.jl を使った自動微分が使える&#40;なんやと．．．？！&#41;</p>
<ul>
<li><p>このスライドでは v0.3.0 を試した速報スライドです.</p>
</li>
<li><p>2022/03/10 時点の話です</p>
</li>
</ul>
<hr />
<h1 id="example"><a href="#example" class="header-anchor">Example</a></h1>
<p><a href="https://github.com/rejuvyesh/PyCallChainRules.jl#example">リポジトリの例から抜粋</a></p>
<pre><code class="julia hljs"><span class="hljs-keyword">using</span> PyCallChainRules.Torch: TorchModuleWrapper, torch
<span class="hljs-keyword">using</span> Zygote

indim = <span class="hljs-number">32</span>
outdim = <span class="hljs-number">16</span>
torch_module = torch.nn.Linear(indim, outdim) <span class="hljs-comment"># Can be anything subclassing torch.nn.Module</span>
jlwrap = TorchModuleWrapper(torch_module)

batchsize = <span class="hljs-number">64</span>
input = randn(<span class="hljs-built_in">Float32</span>, indim, batchsize)
output = jlwrap(input)

target = randn(<span class="hljs-built_in">Float32</span>, outdim, batchsize)
loss(m, x, y) = sum(m(x) .- target)
grad, = Zygote.gradient(m-&gt;loss(m, input, target), jlwrap)</code></pre>
<p><code>TorchModuleWrapper</code> 学習パラメータを持っているレイヤーをラップできている. <code>torch.nn.ReLU</code> はラップできないが Flux.jl の方でカバーできる.</p>
<hr />
<p>class: center, middle</p>
<h1 id="application"><a href="#application" class="header-anchor">Application</a></h1>
<p>みんな大好き MNIST 学習 <a href="https://github.com/terasakisatoshi/PCRP.jl">Code is available</a></p>
<hr />
<h2 id="load_julia_libraries"><a href="#load_julia_libraries" class="header-anchor">Load Julia libraries</a></h2>
<pre><code class="julia hljs"><span class="hljs-keyword">using</span> Random

<span class="hljs-keyword">using</span> Flux
<span class="hljs-keyword">using</span> Flux.Data:DataLoader
<span class="hljs-keyword">using</span> MLDatasets
<span class="hljs-keyword">using</span> Images
<span class="hljs-keyword">using</span> ProgressMeter
<span class="hljs-keyword">using</span> PyCallChainRules.Torch: TorchModuleWrapper, torch

Base.<span class="hljs-meta">@kwdef</span> <span class="hljs-keyword">mutable struct</span> Args
    η = <span class="hljs-number">3e-4</span>
    λ = <span class="hljs-number">0</span>
    batchsize=<span class="hljs-number">128</span>
    epochs=<span class="hljs-number">5</span>
    seed=<span class="hljs-number">12345</span>
    use_cuda=<span class="hljs-literal">false</span>
<span class="hljs-keyword">end</span>

args = Args()
Random.seed!(args.seed)
<span class="hljs-literal">ENV</span>[<span class="hljs-string">&quot;DATADEPS_ALWAYS_ACCEPT&quot;</span>] = <span class="hljs-literal">true</span></code></pre>
<hr />
<h2 id="prepare_mnist_dataset"><a href="#prepare_mnist_dataset" class="header-anchor">Prepare MNIST dataset </a></h2>
<pre><code class="julia hljs"><span class="hljs-keyword">using</span> Flux
<span class="hljs-keyword">using</span> Flux.Data: DataLoader
<span class="hljs-keyword">using</span> MLDatasets

xtrain, ytrain = MLDatasets.MNIST.traindata(<span class="hljs-built_in">Float32</span>)
xtest, ytest = MLDatasets.MNIST.testdata(<span class="hljs-built_in">Float32</span>)
xtrain = Flux.unsqueeze(xtrain, <span class="hljs-number">3</span>) <span class="hljs-comment"># (28, 28, 60000) -&gt; (28, 28, 1 , 60000)</span>
xtest = Flux.unsqueeze(xtest, <span class="hljs-number">3</span>)   <span class="hljs-comment"># (28, 28, 10000) -&gt; (28, 28, 1, 10000)</span>
ytrain = Flux.onehotbatch(ytrain, <span class="hljs-number">0</span>:<span class="hljs-number">9</span>) <span class="hljs-comment"># (60000,) -&gt; (10, 60000)</span>
ytest = Flux.onehotbatch(ytest, <span class="hljs-number">0</span>:<span class="hljs-number">9</span>)   <span class="hljs-comment"># (10000,) -&gt; (10, 10000)</span>
train_loader = DataLoader((xtrain, ytrain), batchsize=<span class="hljs-number">128</span>, shuffle=<span class="hljs-literal">true</span>)
test_loader = DataLoader((xtest, ytest),  batchsize=<span class="hljs-number">128</span>);</code></pre>
<hr />
<h2 id="a_hrefhttpsgithubcomfluxmlmodel-zootreemastervisionconv_mnistcreate_lenet"><a href="#a_hrefhttpsgithubcomfluxmlmodel-zootreemastervisionconv_mnistcreate_lenet" class="header-anchor"><a href="https://github.com/FluxML/model-zoo/tree/master/vision/conv_mnist">Create LeNet</a></a></h2>
<pre><code class="julia hljs"><span class="hljs-keyword">struct</span> LeNet
    cnn_layer
    mlp_layer
    nclasses
<span class="hljs-keyword">end</span>

Flux.<span class="hljs-meta">@functor</span> LeNet (cnn_layer, mlp_layer) <span class="hljs-comment"># cnn_layer と mlp_layer が学習パラメータであることを指定する</span>

(net::LeNet)(x) = x |&gt; net.cnn_layer |&gt; flatten |&gt; net.mlp_layer</code></pre>
<p>ここまでは Flux.jl と同じ</p>
<hr />
<h2 id="constructor"><a href="#constructor" class="header-anchor">Constructor</a></h2>
<pre><code class="julia hljs"><span class="hljs-keyword">function</span> create_model(imsize::<span class="hljs-built_in">Tuple</span>{<span class="hljs-built_in">Int</span>,<span class="hljs-built_in">Int</span>,<span class="hljs-built_in">Int</span>}, nclasses::<span class="hljs-built_in">Int</span>)
    W, H, inC = imsize
    out_conv_size = (W ÷ <span class="hljs-number">4</span> - <span class="hljs-number">3</span>, H ÷ <span class="hljs-number">4</span> - <span class="hljs-number">3</span>, <span class="hljs-number">16</span>)
    cnn_layer = Chain(
        TorchModuleWrapper(torch.nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, (<span class="hljs-number">5</span>,<span class="hljs-number">5</span>))), <span class="hljs-comment"># PyTorch</span>
        x-&gt;Flux.relu.(x), <span class="hljs-comment"># Flux</span>
        MaxPool((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)), <span class="hljs-comment"># Flux</span>
        TorchModuleWrapper(torch.nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, (<span class="hljs-number">5</span>,<span class="hljs-number">5</span>))), <span class="hljs-comment"># PyTorch</span>
        x-&gt;Flux.relu.(x), <span class="hljs-comment"># Flux</span>
        MaxPool((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)) <span class="hljs-comment"># Flux</span>
    )
    mlp_layer = Chain(
        TorchModuleWrapper(torch.nn.Linear(prod(out_conv_size), <span class="hljs-number">120</span>)), <span class="hljs-comment"># PyTorch</span>
        x-&gt;Flux.relu.(x), <span class="hljs-comment"># Flux</span>
        TorchModuleWrapper(torch.nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>)), <span class="hljs-comment"># PyTorch</span>
        x-&gt;Flux.relu.(x), <span class="hljs-comment"># Flux</span>
        TorchModuleWrapper(torch.nn.Linear(<span class="hljs-number">84</span>,nclasses)), <span class="hljs-comment"># PyTorch</span>
    )
    LeNet(cnn_layer, mlp_layer, nclasses) <span class="hljs-comment"># Julia struct</span>
<span class="hljs-keyword">end</span></code></pre>
<p>これでできる...</p>
<hr />
<pre><code class="julia hljs">model = create_model((<span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>), <span class="hljs-number">10</span>) |&gt; f32
ps = Flux.params(model);
opt = Descent(<span class="hljs-number">0.05</span>)
loss(ŷ, y) = Flux.Losses.logitcrossentropy(ŷ, y)

<span class="hljs-comment"># Training loop</span>
<span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:args.epochs
    <span class="hljs-meta">@showprogress</span> <span class="hljs-keyword">for</span> (x, y) <span class="hljs-keyword">in</span> train_loader
        gs, = Flux.gradient(model) <span class="hljs-keyword">do</span> m
            ŷ = m(x)
            loss(ŷ, y)
        <span class="hljs-keyword">end</span>
        cnt = <span class="hljs-number">1</span>
        <span class="hljs-keyword">for</span> g <span class="hljs-keyword">in</span> gs
            isnothing(g) &amp;&amp; <span class="hljs-keyword">continue</span>
            <span class="hljs-keyword">for</span> lay <span class="hljs-keyword">in</span> g.layers
                isnothing(lay) &amp;&amp; <span class="hljs-keyword">continue</span>
                <span class="hljs-keyword">for</span> Δ <span class="hljs-keyword">in</span> lay.params
                    <span class="hljs-comment">#@show Δ</span>
                    ps[cnt] .-= Flux.Optimise.apply!(opt, ps[cnt], Δ)
                    cnt += <span class="hljs-number">1</span>
                <span class="hljs-keyword">end</span>
            <span class="hljs-keyword">end</span>
        <span class="hljs-keyword">end</span>
    <span class="hljs-keyword">end</span>
    acc = sum(Flux.onecold(model(xtest)) .== Flux.onecold(ytest))
    acc /= size(ytest, <span class="hljs-number">2</span>)
    <span class="hljs-meta">@info</span>(<span class="hljs-string">&quot;acc&quot;</span>, <span class="hljs-number">100</span>acc, <span class="hljs-string">&quot;%&quot;</span>)
<span class="hljs-keyword">end</span></code></pre>
<hr />
<h1 id="result"><a href="#result" class="header-anchor">Result</a></h1>
<p>できたできゅ！</p>
<p>&lt;img width&#61;&quot;500&quot; alt&#61;&quot;image&quot; src&#61;&quot;https://user-images.githubusercontent.com/16760547/157697417-997134de-6d2b-4d8b-b4a7-d028350100a2.png&quot;&gt;</p>
<hr />
<p>Flux.jl お作法と完全にコンパチブルではない. よって <code>Optimise.update&#33;</code> を使うことができない.</p>
<p>色々デバックした結果，下記のようにループを書いて対応させると良さそう&#40;?&#41; もうちょっとスマートに描ければ素敵</p>
<pre><code class="julia hljs">gs, = Flux.gradient(model) <span class="hljs-keyword">do</span> m
    ŷ = m(x)
    loss(ŷ, y)
<span class="hljs-keyword">end</span>
cnt = <span class="hljs-number">1</span>
<span class="hljs-keyword">for</span> g <span class="hljs-keyword">in</span> gs
    isnothing(g) &amp;&amp; <span class="hljs-keyword">continue</span>
    <span class="hljs-keyword">for</span> lay <span class="hljs-keyword">in</span> g.layers
        isnothing(lay) &amp;&amp; <span class="hljs-keyword">continue</span>
        <span class="hljs-keyword">for</span> Δ <span class="hljs-keyword">in</span> lay.params
            <span class="hljs-comment">#@show Δ</span>
            ps[cnt] .-= Flux.Optimise.apply!(opt, ps, Δ)
            cnt += <span class="hljs-number">1</span>
        <span class="hljs-keyword">end</span>
    <span class="hljs-keyword">end</span>
<span class="hljs-keyword">end</span></code></pre>
<hr />
<h1 id="question"><a href="#question" class="header-anchor">Question</a></h1>
<ul>
<li><p>GPU でも使える？</p>
<ul>
<li><p>functorch がビルドできればOK. 2 週間前に動作を確認.</p>
</li>
<li><p>安定化は今後期待される</p>
</li>
</ul>
</li>
<li><p><code>Chain&#40;Flux 畳み込み, Torch の畳み込み&#41;</code> はできる？</p>
<ul>
<li><p>割り切って Torch だけで書くというのはアリ？</p>
</li>
<li><p>~~forward 計算はできるけれど Flux.gradient のインターフェースの統一が不十分.~~</p>
<ul>
<li><p>こちらの<a href="https://github.com/rejuvyesh/PyCallChainRules.jl/blob/main/examples/simpleflux/train_ml_mix_explicit.jl">問題は解決されました</a>．</p>
</li>
</ul>
</li>
<li><p>ゆーて PyCallChainRules は v0.3.x なので今後に期待.</p>
</li>
<li><p>実用に向けてIssueで議論が行われている．</p>
</li>
</ul>
</li>
</ul>
<hr />
<h1 id="appendix"><a href="#appendix" class="header-anchor">Appendix</a></h1>
<p>コードはここに置いておきます.</p>
<p>https://github.com/terasakisatoshi/PCRP.jl</p>
<div class="page-foot">
  <div class="copyright">
    &copy; SatoshiTerasaki. Last modified: 2022年5月25日 水曜日. Website built with 
    <a href="http://github.com/terasakisatoshi/MathSeminar.jl">MathSeminar.jl</a>
    <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>.
  </div>
</div>
</div><!-- CONTENT ENDS HERE -->
  </main> <!-- end of id=main -->
  <script src="/MathSeminar.jl/libs/vela/metisMenu.min.js"></script>
  <script src="/MathSeminar.jl/libs/vela/slideout.min.js"></script>
  
  
    


  
</body>
</html>
